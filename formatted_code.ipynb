{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d9dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-270m-it\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ").eval()\n",
    "\n",
    "model_8bitq = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = quantization_config\n",
    ").eval()\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model_4bitq = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68bb5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\tokenizer\")\n",
    "model.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model\")\n",
    "model_8bitq.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model_8bitq\")\n",
    "model_4bitq.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model_4bitq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa14f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb0675ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tensors: 236\n",
      "\n",
      "Tensor name: model.embed_tokens.weight, dtype: torch.float32, shape: torch.Size([262144, 640])\n",
      "Tensor name: model.layers.0.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.0.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.1.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.10.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.11.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.12.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.13.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.14.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.15.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.16.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.17.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.2.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.3.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.4.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.5.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.6.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.7.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.8.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.9.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.norm.weight, dtype: torch.float32, shape: torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model/model.safetensors\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    keys = list(f.keys())\n",
    "    print(f\"Total number of tensors: {len(keys)}\\n\")\n",
    "    \n",
    "    for key in keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2140a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tensors: 488\n",
      "\n",
      "Tensor name: model.embed_tokens.weight, dtype: torch.float16, shape: torch.Size([262144, 640])\n",
      "Tensor name: model.layers.0.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.norm.weight, dtype: torch.float16, shape: torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_8bitq/model.safetensors\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    keys = list(f.keys())\n",
    "    print(f\"Total number of tensors: {len(keys)}\\n\")\n",
    "    \n",
    "    for key in keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6488807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tensors: 614\n",
      "\n",
      "Tensor name: model.embed_tokens.weight, dtype: torch.float16, shape: torch.Size([262144, 640])\n",
      "Tensor name: model.layers.0.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.1.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.10.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.11.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.12.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.13.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.14.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.15.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.16.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.17.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.2.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.3.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.4.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.5.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.6.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.7.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.8.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.9.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.norm.weight, dtype: torch.float16, shape: torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_4bitq/model.safetensors\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    keys = list(f.keys())\n",
    "    print(f\"Total number of tensors: {len(keys)}\\n\")\n",
    "    \n",
    "    for key in keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51a14749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072393732\n",
      "435927298\n",
      "385792258\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint())\n",
    "print(model_8bitq.get_memory_footprint())\n",
    "print(model_4bitq.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a3230ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "tensor([[ 0.0026,  0.0125, -0.0148,  ..., -0.0024,  0.0369,  0.0162],\n",
      "        [-0.0032, -0.0031, -0.0146,  ..., -0.0066, -0.0359, -0.0045],\n",
      "        [-0.0388, -0.0079, -0.0167,  ...,  0.0097,  0.0054,  0.0496],\n",
      "        ...,\n",
      "        [-0.0220,  0.0062, -0.0018,  ..., -0.0095,  0.0047, -0.0020],\n",
      "        [ 0.0148,  0.0131, -0.0320,  ...,  0.0036,  0.0080, -0.0043],\n",
      "        [ 0.0020, -0.0002,  0.0069,  ...,  0.0118,  0.0067, -0.0120]])\n",
      "First 10 values: tensor([ 0.0026,  0.0125, -0.0148,  0.0182,  0.0282, -0.0091,  0.0188, -0.0004,\n",
      "        -0.0233, -0.0071])\n"
     ]
    }
   ],
   "source": [
    "# Print the values of a specific tensor from safetensors file\n",
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model/model.safetensors\"\n",
    "tensor_key = \"model.layers.0.mlp.down_proj.weight\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    tensor = f.get_tensor(tensor_key)\n",
    "    print(f\"Tensor name: {tensor_key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")\n",
    "    print(tensor)  # Print all values (may be large)\n",
    "    # Optionally, print only some elements\n",
    "    print(\"First 10 values:\", tensor.flatten()[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac1a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor name: model.layers.0.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "First 10 values: [0.076171875, 0.08642578125, 0.08154296875, 0.0849609375, 0.10302734375, 0.0810546875, 0.09033203125, 0.08642578125, 0.087890625, 0.09912109375]\n",
      "----------------------------------------\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "First 10 values: [4, 21, -25, 30, 47, -15, 31, -1, -39, -12]\n",
      "----------------------------------------\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "First 10 values: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_8bitq/model.safetensors\"\n",
    "tensor_keys = [\n",
    "    \"model.layers.0.mlp.down_proj.SCB\",\n",
    "    \"model.layers.0.mlp.down_proj.weight\",\n",
    "    \"model.layers.0.mlp.down_proj.weight_format\"\n",
    "]\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    for key in tensor_keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")\n",
    "        print(\"First 10 values:\", tensor.flatten()[:10].tolist() if tensor.numel() > 10 else tensor.tolist())\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd3adf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([640, 2048])\n",
      "mean abs diff: 0.00017775085871107876\n",
      "max abs diff: 0.002891242504119873\n",
      "first 10 dequantized: tensor([ 0.0024,  0.0126, -0.0150,  0.0180,  0.0282, -0.0090,  0.0186, -0.0006,\n",
      "        -0.0234, -0.0072])\n",
      "first 10 fp32:        tensor([ 0.0026,  0.0125, -0.0148,  0.0182,  0.0282, -0.0091,  0.0188, -0.0004,\n",
      "        -0.0233, -0.0071])\n",
      "torch.Size([640])\n",
      "torch.Size([640, 2048])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "model_dir = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model/model.safetensors\"\n",
    "quant_model_dir = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_8bitq/model.safetensors\"\n",
    "\n",
    "key = \"model.layers.0.mlp.down_proj.weight\"\n",
    "with safe_open(model_dir, framework =\"pt\") as f :\n",
    "    w_fp32 = f.get_tensor(key).float()\n",
    "\n",
    "with safe_open(quant_model_dir, framework = \"pt\") as f :\n",
    "    w_q = f.get_tensor(key).to(torch.int8)\n",
    "    scb = f.get_tensor(key.replace(\"weight\", \"SCB\")).float()\n",
    "    fmt = int(f.get_tensor(key.replace(\"weight\", \"weight_format\")).item())\n",
    "\n",
    "assert fmt == 0, \"This snippet assumes weight_format == 0 (symmetric per-row).\"\n",
    "\n",
    "scale = scb / 127.0  # per-row scale\n",
    "w_deq = w_q.float() * scale[:, None]\n",
    "\n",
    "diff = (w_deq - w_fp32).abs()\n",
    "print(\"shape:\", w_fp32.shape)\n",
    "print(\"mean abs diff:\", diff.mean().item())\n",
    "print(\"max abs diff:\", diff.max().item())\n",
    "print(\"first 10 dequantized:\", w_deq.flatten()[:10])\n",
    "print(\"first 10 fp32:       \", w_fp32.flatten()[:10])\n",
    "\n",
    "\n",
    "#there is a scale for each column of the tensor : \n",
    "print(scb.shape) \n",
    "print(w_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15b3d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model output:  Once upon a time, there was a small, unassuming house. It was built of weathered wood, with a single,\n",
      "FP32 inference time (s): 2.3690\n",
      "\n",
      "8-bit model output:  Once upon a time\n",
      "8-bit (quantized) inference time (s): 3.4226\n",
      "\n",
      "(Speedup ratio: FP32 / 8-bit = 0.69)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "max_new_tokens = 20\n",
    "\n",
    "# Tokenize prompt with attention mask and set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "input_ids = inputs[\"input_ids\"].to(\"cpu\")\n",
    "attention_mask = inputs[\"attention_mask\"].to(\"cpu\")\n",
    "\n",
    "# Make sure everything runs on CPU\n",
    "model = model.to(\"cpu\")\n",
    "model_8bitq = model_8bitq\n",
    "\n",
    "# Optionally clear unsupported generation flags for Gemma3\n",
    "try:\n",
    "    model.generation_config.top_p = None\n",
    "    model.generation_config.top_k = None\n",
    "    model_8bitq.generation_config.top_p = None\n",
    "    model_8bitq.generation_config.top_k = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Warm-up to populate caches\n",
    "with torch.inference_mode():\n",
    "    _ = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    _ = model_8bitq.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# FP32 timing\n",
    "with torch.inference_mode():\n",
    "    start_fp32 = time.time()\n",
    "    output_fp32 = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    end_fp32 = time.time()\n",
    "\n",
    "# 8-bit model timing\n",
    "with torch.inference_mode():\n",
    "    start_8bit = time.time()\n",
    "    output_8bitq = model_8bitq.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    end_8bit = time.time()\n",
    "\n",
    "print(\"FP32 model output: \", tokenizer.decode(output_fp32[0], skip_special_tokens=True))\n",
    "print(f\"FP32 inference time (s): {end_fp32 - start_fp32:.4f}\")\n",
    "\n",
    "print(\"\\n8-bit model output: \", tokenizer.decode(output_8bitq[0], skip_special_tokens=True))\n",
    "print(f\"8-bit (quantized) inference time (s): {end_8bit - start_8bit:.4f}\")\n",
    "\n",
    "# Report relative speed\n",
    "if (end_8bit - start_8bit) > 0:\n",
    "    speedup = (end_fp32 - start_fp32) / (end_8bit - start_8bit)\n",
    "    print(f\"\\n(Speedup ratio: FP32 / 8-bit = {speedup:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed3173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No Real 8-bit Inference Support on CPU\n",
    "# bitsandbytes is mainly designed for use with NVIDIA GPUs, and its quantized kernels (8-bit and 4-bit) are only fast/accurate on CUDA devices.\n",
    "# On CPU, bitsandbytes can still quantize and dequantize tensors, so you can store and load int8 weightsbut it does NOT have efficient, native int8 matrix multiplication for inference.\n",
    "# Instead, it falls back to internal reconstruction and/or PyTorch logic, which is not optimized and may even process data incorrectly (typically slower, but sometimes incorrect bias or matmul implementations for quantized tensors on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91966e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate to French: 'How are you today?'\n",
      "\n",
      "**Translation:**\n",
      "\n",
      "*   **Option 1 (Most common):**  \"Comment allez-vous aujourd'hui ?\"\n",
      "*   **Option 2 (More formal):** \"Comment allez-vous aujourd'hui ?\"\n",
      "*   **Option 3 (More casual):** \"Comment allez-vous\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate to French: 'How are you today?'\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20953b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Translate to Spanish: 'I love machine learning.'\n",
      "model\n",
      "The translation of \"I love machine learning\" is:\n",
      "\n",
      "* **Me encanta el aprendizaje automtico.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\":\"user\",\"content\":\"Translate to Spanish: 'I love machine learning.'\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e28d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
