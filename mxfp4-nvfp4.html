<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MXFP4 & NVFP4: Efficient 4-bit Floating Points for AI/LLM Inference</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        .question { font-weight: bold; font-size: 1.15em; margin: 38px 0 10px 0; }
        .qaspacing { margin-bottom: 32px; }
        .qa-intro { margin: 28px 0 40px 0; background: #fff9f0; border-left: 6px solid #e69742; padding: 20px 32px; border-radius:7px; }
        .qa-intro li { padding: 4px 0; font-size: 1.11em; }
        @media (max-width: 700px) {.qa-intro{padding:12px 10px}}
    </style>
</head>
<body>
    <div class="container">
        <div class="content-box">
            <h1 class="page-title">MXFP4 & NVFP4: Efficient 4-bit Floating Point Formats for LLM Inference</h1>
            <div style="margin-bottom:24px"><a class="nav-link" href="blog.html">← Back to Blog</a></div>
            <p class="date">Published: October 2025</p>
            <div class="qa-intro">
                <p><b>In this post, I’ll answer these key questions about MXFP4/NVFP4 for LLM inference:</b></p>
                <ul>
                    <li>1. <b>Why do microscaled 4-bit floating point formats work for AI inference? What is their main property?</b></li>
                    <li>2. <b>Isn't dequantization overhead high with an added step—how does it help over BFloat16?</b></li>
                    <li>3. <b>What kind of hardware supports MXFP4/NVFP4?</b></li>
                    <li>4. <b>What are their key drawbacks?</b></li>
                    <li>5. <b>Is there proof that these actually work on real LLMs?</b></li>
                </ul>
            </div>

            <div class="question">1. Why do microscaled 4-bit floating point formats work in implementation? What property is being exploited by them and what are these formats?</div>
            <div class="qaspacing">
                <p><b>Core Property:</b> MXFP4 and NVFP4 take advantage of a key neural network trait: <b>Locally, values within tensors are similar in magnitude.</b> Instead of wild outliers being spread across the whole tensor, values generally cluster in tight ranges when looked at in small blocks. This makes <b>block-wise quantization</b> accurate and efficient.</p>
                <ul>
                    <li><b>MXFP4:</b> Divides tensors into 32-value blocks using a shared, power-of-two scale factor (E8M0); individual values are 4-bits (E2M1). Dequantization: <code>dequantized_value = FP4_value × scale_factor</code></li>
                    <li><b>NVFP4:</b> Uses smaller, 16-value blocks and a fractional, higher-precision E4M3 FP8 scale per block, for much finer local adaptation and reduced error. A second global FP32 scale covers the tensor’s overall range.</li>
                </ul>
            </div>

            <div class="question">2. Isn't the number of arithmetic operations higher (dequantizing each value)—does this extra step cost more than using BFloat16?</div>
            <div class="qaspacing">
                <p><b>Not really!</b> For LLMs, <b>memory access is the bottleneck</b> (not computation). BFloat16 models (for 70B params) move 140GB of data per pass; FP4 models move only about 40GB!</p>
                <ul>
                    <li><b>Dequantization overhead:</b> Adds ~6% to compute steps, but GPUs spend 80-90% of time just moving data. Modern tensor cores <b>fuse</b> scaling with multiplication (GEMM loads), so the true impact is &lt;1% of total time.</li>
                </ul>
            </div>

            <div class="question">3. What kind of hardware support exists for MXFP4/NVFP4 formats?</div>
            <div class="qaspacing">
                <ul>
                    <li><b>MXFP4:</b> Supported on NVIDIA Hopper+ (H100, H200, GH200, B100, B200, GB200, plus RTX 50-series/Pro Blackwell). <br><b>Not supported:</b> RTX 4090, A100, or older cards.</li>
                    <li><b>NVFP4:</b> <b>Blackwell-only (B100/B200/RTX 50+).</b> Needs fifth-gen tensor cores for FP4 ops/scaling.</li>
                </ul>
            </div>

            <div class="question">4. What are the drawbacks of these formats?</div>
            <div class="qaspacing">
                <ul>
                    <li><b>NVFP4:</b> Very small groups, so normal outlier-smoothing tricks don’t work as well.</li>
                    <li><b>MXFP4:</b> Power-of-two scaling can cause accuracy loss due to coarse scaling steps.</li>
                </ul>
            </div>

            <div class="question">5. Is there proof that these formats work well?</div>
            <div class="qaspacing">
                <ul>
                    <li>GPT-OSS models were quantized to MXFP4, with strong accuracy and major speedups (see below for references).</li>
                    <li>Official model card and blog posts demonstrate MXFP4 results.</li>
                </ul>
                <em>(Paste specific results from the referenced paper/model cards here for detailed proof points.)</em>
            </div>

            <h2>References & Further Reading</h2>
            <ul>
                <li><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" target="_blank" rel="noopener noreferrer">OpenAI GPT-OSS Model Card (Official, MXFP4 Accuracy Results)</a></li>
                <li><a href="https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/" target="_blank" rel="noopener noreferrer">NVIDIA Developer Blog: Introducing NVFP4</a></li>
                <li><a href="https://github.com/IST-DASLab/FP-Quant" target="_blank" rel="noopener noreferrer">Github: IST-DASLab/FP-Quant</a></li>
            </ul>
        </div>
    </div>
</body>
</html>
