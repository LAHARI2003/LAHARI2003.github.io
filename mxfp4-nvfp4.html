<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MXFP4 & NVFP4: In-Depth Technical FAQ</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        .question { font-weight: bold; font-size: 1.13em; margin-top:32px; margin-bottom: 10px; }
        .answer-block { margin-bottom: 32px; line-height: 1.7; }
        .ref-section { margin-top: 36px; }
        .md-section { margin-top: 26px; }
        ul, ol { margin-bottom: 0.3em; margin-top: 0.3em; }
    </style>
</head>
<body>
<div class="container">
  <div class="content-box">
    <h1 class="page-title">MXFP4 & NVFP4: Microscaled 4-bit Floating Point Formats for Modern AI</h1>
    <div style="margin-bottom:24px"><a class="nav-link" href="blog.html">← Back to Blog</a></div>
    <p class="date">Published: October 2025</p>

    <div class="md-section">
      <p><b>Simple Questions on Why and How microscaled 4-bit floating point formats—MXFP4 and NVFP4—work:</b></p>
    </div>

    <div class="md-section">
      <div class="question">Why do these numerical formats work in practice? What property is being exploited, and what exactly are MXFP4 and NVFP4?</div>
      <div class="answer-block">
        <p><b>The Core Property:</b> MXFP4 and NVFP4 leverage a fundamental feature of neural network weights and activations: <b>values exhibit locally similar magnitudes along contiguous tensor dimensions.</b> Rather than facing extreme outliers scattered throughout tensors, values tend to cluster within specific ranges when analyzed in small neighborhoods. This <b>locality property</b> makes block-wise quantization highly effective for both storage and computation.</p>

        <p><b>MXFP4: Power-of-Two Block Scaling</b><br>
        MXFP4 divides tensors into blocks of 32 contiguous elements, where each block shares a single scaling factor stored in E8M0 format (8-bit exponent-only, no mantissa). This power-of-two scaling works as follows:</p>
        <ul>
          <li>For each block, the scale is computed from the block's maximum absolute value, allowing dynamic local adaptation.</li>
          <li>Each 4-bit value uses an <b>E2M1 representation</b> (2 exponent bits, 1 mantissa bit, 1 sign bit), covering approximately -6 to +6 in normalized form.</li>
          <li>The dequantized value is: <code>dequantized_value = FP4_value × scale_factor</code></li>
        </ul>

        <p><b>NVFP4: Fine-Grained Fractional Scaling</b><br>
        NVFP4 enhances this method with two critical improvements:</p>
        <ul>
          <li><b>Smaller block size:</b> Block size is reduced from 32 to 16 for finer local adaptation and more frequent scaling opportunities. This enables precise capture of local maxima, significantly reducing the impact of mini-outliers.</li>
          <li><b>High-precision scaling:</b> NVFP4 utilizes E4M3 FP8 for scale factors instead of E8M0, enabling non-power-of-two scaling with fractional precision. The 3-bit mantissa offers 8× more scaling levels between powers of two, reducing quantization error by better matching the true data distribution.</li>
          <li><b>Two-level scaling hierarchy:</b> Each 16-value micro-block has an E4M3 scale, plus a global FP32 scalar applied to the entire tensor. This dual approach minimizes errors across both local and global tensor ranges.</li>
        </ul>
      </div>
    </div>

    <div class="md-section">
      <div class="question">Intuitively, doesn't requiring an extra dequantization step mean more arithmetic operations? If so, how does this actually benefit us compared to the commonly-used BFloat16?</div>
      <div class="answer-block">
        <p><b>LLM Inference is Memory-Bound, Not Compute-Bound</b></p>
        <p>The key insight: Modern GPUs spend 80–90% of time moving data from memory, not performing computation. With BFloat16, each parameter transfer requires 16 bits from HBM memory to GPU cores. For a 70B model, that results in 140GB of data movement per forward pass! FP4 reduces this to roughly 40GB—a 3.5× drop in memory bandwidth use.</p>

        <p><b>The Dequantization Overhead is Negligible</b><br>
        While dequantization adds an extra scaling step, the additional operations amount to about 6% more arithmetic—but this operates on the small (10–20%) fraction of time spent doing compute. Modern tensor cores perform the dequantization <b>fused</b> into the GEMM kernel; scaling occurs as values load into registers, not as a separate operation. As a result, the total execution time increase is typically &lt;1%.</p>
      </div>
    </div>

    <div class="md-section">
      <div class="question">What kind of hardware support exists for MXFP4 and NVFP4 formats?</div>
      <div class="answer-block">
        <p><b>MXFP4 Hardware Requirements</b><br>
        MXFP4 requires NVIDIA GPUs with compute capability 9.0 or higher. This restricts support to recent architectures:</p>
        <ul>
          <li><b>Datacenter GPUs:</b> H100 (80GB/90GB), H200 (141GB HBM3e), and GH200 Grace Hopper Superchip, all using Hopper. Newer Blackwell datacenter GPUs (B100, B200, GB200) also support MXFP4.</li>
          <li><b>Consumer GPUs:</b> GeForce RTX 50 series (5090, 5080, 5070 Ti, 5070, 5060 Ti, 5060) and RTX Pro Blackwell workstation cards bring MXFP4 to desktops.</li>
        </ul>
        <p><b>Important Note:</b> Popular GPUs such as RTX 4090, A100, and RTX 3090 are <b>not supported</b> for MXFP4 models—they lack the required new-generation tensor cores.</p>

        <p><b>NVFP4 Hardware Requirements</b><br>
        NVFP4 is exclusive to Blackwell architecture (compute capability 10.0+), leveraging fifth-generation Tensor Cores that support native FP4 ops and dynamic scaling:</p>
        <ul>
          <li><b>Blackwell GPUs:</b> B100 and B200 deliver 20 PFLOPS FP4 performance with specialized tensor cores covering scaling and grouping automatically.</li>
          <li><b>Blackwell Ultra:</b> GB300/B300 delivers 50% better NVFP4 performance than standard Blackwell, with 288GB HBM3e memory and improved efficiency.</li>
          <li><b>Consumer Blackwell:</b> GeForce RTX 50 series brings up to 4000 AI TOPS with FP4 capabilities for local inference.</li>
        </ul>
      </div>
    </div>

    <div class="md-section">
      <div class="question">What are the drawbacks of these formats?</div>
      <div class="answer-block">
        <ol>
          <li><b>NVFP4:</b> Its very small group size neutralizes outlier mitigation techniques traditionally used with larger quantization groups.</li>
          <li><b>MXFP4:</b> The power-of-two-only scale quantization can significantly degrade accuracy, due to high error from coarse scaling.</li>
        </ol>
      </div>
    </div>

    <div class="md-section">
      <div class="question">Proofs that these formats work:</div>
      <div class="answer-block">
        <ul>
          <li>GPT-OSS models have been post-training quantized (PTQ) using the MXFP4 format, showing strong accuracy and inference speedup.</li>
          <li>The checkpoint referenced in the official model card uses MXFP4; evaluations and blog results for these models are based on MXFP4 quantization.</li>
        </ul>
        <p><b>Paste the results from the primary paper below to demonstrate the performance and accuracy achieved with these quantization algorithms.</b></p>
      </div>
    </div>

    <div class="ref-section">
      <div class="question">Reference Links</div>
      <ul>
        <li><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" target="_blank" rel="noopener noreferrer">OpenAI GPT-OSS Model Card (PDF)</a></li>
        <li><a href="https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/" target="_blank" rel="noopener noreferrer">NVIDIA Developer Blog: Introducing NVFP4</a></li>
        <li><a href="https://github.com/IST-DASLab/FP-Quant" target="_blank" rel="noopener noreferrer">GitHub: IST-DASLab/FP-Quant</a></li>
      </ul>
    </div>
  </div>
</div>
</body>
</html>
