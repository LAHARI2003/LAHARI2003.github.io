{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d9dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-270m-it\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ").eval()\n",
    "\n",
    "model_8bitq = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = quantization_config\n",
    ").eval()\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model_4bitq = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68bb5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\tokenizer\")\n",
    "model.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model\")\n",
    "model_8bitq.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model_8bitq\")\n",
    "model_4bitq.save_pretrained(r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model_4bitq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01451411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa14f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb0675ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tensors: 236\n",
      "\n",
      "Tensor name: model.embed_tokens.weight, dtype: torch.float32, shape: torch.Size([262144, 640])\n",
      "Tensor name: model.layers.0.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.0.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.1.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.10.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.11.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.12.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.13.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.14.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.15.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.16.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.17.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.2.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.3.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.4.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.5.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.6.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.7.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.8.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.input_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight, dtype: torch.float32, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.post_attention_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.post_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.pre_feedforward_layernorm.weight, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.k_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight, dtype: torch.float32, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.9.self_attn.q_norm.weight, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight, dtype: torch.float32, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight, dtype: torch.float32, shape: torch.Size([256, 640])\n",
      "Tensor name: model.norm.weight, dtype: torch.float32, shape: torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model/model.safetensors\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    keys = list(f.keys())\n",
    "    print(f\"Total number of tensors: {len(keys)}\\n\")\n",
    "    \n",
    "    for key in keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2140a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tensors: 488\n",
      "\n",
      "Tensor name: model.embed_tokens.weight, dtype: torch.float16, shape: torch.Size([262144, 640])\n",
      "Tensor name: model.layers.0.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.mlp.up_proj.SCB, dtype: torch.float32, shape: torch.Size([2048])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight, dtype: torch.int8, shape: torch.Size([2048, 640])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight, dtype: torch.int8, shape: torch.Size([640, 1024])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.SCB, dtype: torch.float32, shape: torch.Size([1024])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight, dtype: torch.int8, shape: torch.Size([1024, 640])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.SCB, dtype: torch.float32, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight, dtype: torch.int8, shape: torch.Size([256, 640])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "Tensor name: model.norm.weight, dtype: torch.float16, shape: torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_8bitq/model.safetensors\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    keys = list(f.keys())\n",
    "    print(f\"Total number of tensors: {len(keys)}\\n\")\n",
    "    \n",
    "    for key in keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6488807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tensors: 614\n",
      "\n",
      "Tensor name: model.embed_tokens.weight, dtype: torch.float16, shape: torch.Size([262144, 640])\n",
      "Tensor name: model.layers.0.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.0.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.0.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.1.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.1.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.1.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.10.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.10.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.10.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.11.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.11.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.11.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.12.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.12.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.12.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.13.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.13.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.13.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.14.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.14.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.14.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.15.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.15.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.15.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.16.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.16.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.16.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.17.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.17.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.17.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.2.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.2.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.2.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.3.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.3.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.3.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.4.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.4.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.4.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.5.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.5.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.5.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.6.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.6.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.6.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.7.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.7.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.7.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.8.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.8.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.8.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.9.input_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight, dtype: torch.uint8, shape: torch.Size([655360, 1])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([20480])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.post_attention_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.post_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.pre_feedforward_layernorm.weight, dtype: torch.float16, shape: torch.Size([640])\n",
      "Tensor name: model.layers.9.self_attn.k_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.k_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.o_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.self_attn.q_norm.weight, dtype: torch.float16, shape: torch.Size([256])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight, dtype: torch.uint8, shape: torch.Size([327680, 1])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([10240])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.q_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([80])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight, dtype: torch.uint8, shape: torch.Size([81920, 1])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight.absmax, dtype: torch.float32, shape: torch.Size([2560])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight.quant_map, dtype: torch.float32, shape: torch.Size([16])\n",
      "Tensor name: model.layers.9.self_attn.v_proj.weight.quant_state.bitsandbytes__fp4, dtype: torch.uint8, shape: torch.Size([79])\n",
      "Tensor name: model.norm.weight, dtype: torch.float16, shape: torch.Size([640])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_4bitq/model.safetensors\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    keys = list(f.keys())\n",
    "    print(f\"Total number of tensors: {len(keys)}\\n\")\n",
    "    \n",
    "    for key in keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51a14749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072393732\n",
      "435927298\n",
      "385792258\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint())\n",
    "print(model_8bitq.get_memory_footprint())\n",
    "print(model_4bitq.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a3230ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.float32, shape: torch.Size([640, 2048])\n",
      "tensor([[ 0.0026,  0.0125, -0.0148,  ..., -0.0024,  0.0369,  0.0162],\n",
      "        [-0.0032, -0.0031, -0.0146,  ..., -0.0066, -0.0359, -0.0045],\n",
      "        [-0.0388, -0.0079, -0.0167,  ...,  0.0097,  0.0054,  0.0496],\n",
      "        ...,\n",
      "        [-0.0220,  0.0062, -0.0018,  ..., -0.0095,  0.0047, -0.0020],\n",
      "        [ 0.0148,  0.0131, -0.0320,  ...,  0.0036,  0.0080, -0.0043],\n",
      "        [ 0.0020, -0.0002,  0.0069,  ...,  0.0118,  0.0067, -0.0120]])\n",
      "First 10 values: tensor([ 0.0026,  0.0125, -0.0148,  0.0182,  0.0282, -0.0091,  0.0188, -0.0004,\n",
      "        -0.0233, -0.0071])\n"
     ]
    }
   ],
   "source": [
    "# Print the values of a specific tensor from safetensors file\n",
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model/model.safetensors\"\n",
    "tensor_key = \"model.layers.0.mlp.down_proj.weight\"\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    tensor = f.get_tensor(tensor_key)\n",
    "    print(f\"Tensor name: {tensor_key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")\n",
    "    print(tensor)  # Print all values (may be large)\n",
    "    # Optionally, print only some elements\n",
    "    print(\"First 10 values:\", tensor.flatten()[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac1a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor name: model.layers.0.mlp.down_proj.SCB, dtype: torch.float32, shape: torch.Size([640])\n",
      "First 10 values: [0.076171875, 0.08642578125, 0.08154296875, 0.0849609375, 0.10302734375, 0.0810546875, 0.09033203125, 0.08642578125, 0.087890625, 0.09912109375]\n",
      "----------------------------------------\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight, dtype: torch.int8, shape: torch.Size([640, 2048])\n",
      "First 10 values: [4, 21, -25, 30, 47, -15, 31, -1, -39, -12]\n",
      "----------------------------------------\n",
      "Tensor name: model.layers.0.mlp.down_proj.weight_format, dtype: torch.uint8, shape: torch.Size([])\n",
      "First 10 values: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_8bitq/model.safetensors\"\n",
    "tensor_keys = [\n",
    "    \"model.layers.0.mlp.down_proj.SCB\",\n",
    "    \"model.layers.0.mlp.down_proj.weight\",\n",
    "    \"model.layers.0.mlp.down_proj.weight_format\"\n",
    "]\n",
    "\n",
    "with safe_open(filename, framework=\"pt\") as f:\n",
    "    for key in tensor_keys:\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"Tensor name: {key}, dtype: {tensor.dtype}, shape: {tensor.shape}\")\n",
    "        print(\"First 10 values:\", tensor.flatten()[:10].tolist() if tensor.numel() > 10 else tensor.tolist())\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd3adf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([640, 2048])\n",
      "mean abs diff: 0.00017775085871107876\n",
      "max abs diff: 0.002891242504119873\n",
      "first 10 dequantized: tensor([ 0.0024,  0.0126, -0.0150,  0.0180,  0.0282, -0.0090,  0.0186, -0.0006,\n",
      "        -0.0234, -0.0072])\n",
      "first 10 fp32:        tensor([ 0.0026,  0.0125, -0.0148,  0.0182,  0.0282, -0.0091,  0.0188, -0.0004,\n",
      "        -0.0233, -0.0071])\n",
      "torch.Size([640])\n",
      "torch.Size([640, 2048])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "model_dir = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model/model.safetensors\"\n",
    "quant_model_dir = \"C:/AI_HW/DL_PRAC/Gemma/gemma3-270m/model_8bitq/model.safetensors\"\n",
    "\n",
    "key = \"model.layers.0.mlp.down_proj.weight\"\n",
    "with safe_open(model_dir, framework =\"pt\") as f :\n",
    "    w_fp32 = f.get_tensor(key).float()\n",
    "\n",
    "with safe_open(quant_model_dir, framework = \"pt\") as f :\n",
    "    w_q = f.get_tensor(key).to(torch.int8)\n",
    "    scb = f.get_tensor(key.replace(\"weight\", \"SCB\")).float()\n",
    "    fmt = int(f.get_tensor(key.replace(\"weight\", \"weight_format\")).item())\n",
    "\n",
    "assert fmt == 0, \"This snippet assumes weight_format == 0 (symmetric per-row).\"\n",
    "\n",
    "scale = scb / 127.0  # per-row scale\n",
    "w_deq = w_q.float() * scale[:, None]\n",
    "\n",
    "diff = (w_deq - w_fp32).abs()\n",
    "print(\"shape:\", w_fp32.shape)\n",
    "print(\"mean abs diff:\", diff.mean().item())\n",
    "print(\"max abs diff:\", diff.max().item())\n",
    "print(\"first 10 dequantized:\", w_deq.flatten()[:10])\n",
    "print(\"first 10 fp32:       \", w_fp32.flatten()[:10])\n",
    "\n",
    "\n",
    "#there is a scale for each column of the tensor : \n",
    "print(scb.shape) \n",
    "print(w_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15b3d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model output:  Once upon a time, there was a small, unassuming house. It was built of weathered wood, with a single, large window that offered a view of the sprawling garden. The house was filled with the scent of freshly cut grass and the gentle hum of bees. It was a place of quiet contemplation and peaceful enjoyment.\n",
      "\n",
      "The house was a haven for many people, and it was a place where they could relax and enjoy the simple pleasures of life.\n",
      "\n",
      "The house was a symbol of a bygone era, a place\n",
      "FP32 inference time (s): 11.5597\n",
      "\n",
      "8-bit model output:  Once upon a time\n",
      "8-bit (quantized) inference time (s): 25.2323\n",
      "\n",
      "(Speedup ratio: FP32 / 8-bit = 0.46)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "max_new_tokens = 100\n",
    "\n",
    "# Tokenize prompt with attention mask and set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "input_ids = inputs[\"input_ids\"].to(\"cpu\")\n",
    "attention_mask = inputs[\"attention_mask\"].to(\"cpu\")\n",
    "\n",
    "# Make sure everything runs on CPU\n",
    "model = model.to(\"cpu\")\n",
    "model_8bitq = model_8bitq\n",
    "\n",
    "# Optionally clear unsupported generation flags for Gemma3\n",
    "try:\n",
    "    model.generation_config.top_p = None\n",
    "    model.generation_config.top_k = None\n",
    "    model_8bitq.generation_config.top_p = None\n",
    "    model_8bitq.generation_config.top_k = None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Warm-up to populate caches\n",
    "with torch.inference_mode():\n",
    "    _ = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    _ = model_8bitq.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# FP32 timing\n",
    "with torch.inference_mode():\n",
    "    start_fp32 = time.time()\n",
    "    output_fp32 = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    end_fp32 = time.time()\n",
    "\n",
    "# 8-bit model timing\n",
    "with torch.inference_mode():\n",
    "    start_8bit = time.time()\n",
    "    output_8bitq = model_8bitq.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=False,              # or True, either way EOS is banned\n",
    "    bad_words_ids=[[tokenizer.eos_token_id]],  # ban EOS token\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    end_8bit = time.time()\n",
    "\n",
    "print(\"FP32 model output: \", tokenizer.decode(output_fp32[0], skip_special_tokens=True))\n",
    "print(f\"FP32 inference time (s): {end_fp32 - start_fp32:.4f}\")\n",
    "\n",
    "print(\"\\n8-bit model output: \", tokenizer.decode(output_8bitq[0], skip_special_tokens=True))\n",
    "print(f\"8-bit (quantized) inference time (s): {end_8bit - start_8bit:.4f}\")\n",
    "\n",
    "# Report relative speed\n",
    "if (end_8bit - start_8bit) > 0:\n",
    "    speedup = (end_fp32 - start_fp32) / (end_8bit - start_8bit)\n",
    "    print(f\"\\n(Speedup ratio: FP32 / 8-bit = {speedup:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed3173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No Real 8-bit Inference Support on CPU\n",
    "# bitsandbytes is mainly designed for use with NVIDIA GPUs, and its quantized kernels (8-bit and 4-bit) are only fast/accurate on CUDA devices.\n",
    "# On CPU, bitsandbytes can still quantize and dequantize tensors, so you can store and load int8 weightsbut it does NOT have efficient, native int8 matrix multiplication for inference.\n",
    "# Instead, it falls back to internal reconstruction and/or PyTorch logic, which is not optimized and may even process data incorrectly (typically slower, but sometimes incorrect bias or matmul implementations for quantized tensors on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91966e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate to French: 'How are you today?'\n",
      "\n",
      "**Translation:**\n",
      "\n",
      "*   **Option 1 (Most common):**  \"Comment allez-vous aujourd'hui ?\"\n",
      "*   **Option 2 (More formal):** \"Comment allez-vous aujourd'hui ?\"\n",
      "*   **Option 3 (More casual):** \"Comment allez-vous\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate to French: 'How are you today?'\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20953b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Translate to Spanish: 'I love machine learning.'\n",
      "model\n",
      "The translation of \"I love machine learning\" is:\n",
      "\n",
      "* **Me encanta el aprendizaje automtico.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\":\"user\",\"content\":\"Translate to Spanish: 'I love machine learning.'\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8399aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# ---------- Loading helpers ----------\n",
    "def load_safetensors_to_dict(path: str) -> Dict[str, torch.Tensor]:\n",
    "    d = {}\n",
    "    with safe_open(path, framework=\"pt\") as f:\n",
    "        for k in f.keys():\n",
    "            d[k] = f.get_tensor(k).detach().cpu()\n",
    "    return d\n",
    "\n",
    "def load_torch_state_dict(path: str) -> Dict[str, torch.Tensor]:\n",
    "    sd = torch.load(path, map_location='cpu')\n",
    "    # if checkpoint has 'state_dict' key (common), unwrap\n",
    "    if isinstance(sd, dict) and \"state_dict\" in sd:\n",
    "        sd = sd[\"state_dict\"]\n",
    "    return {k: v.detach().cpu() for k, v in sd.items()}\n",
    "\n",
    "# ---------- Comparison metrics ----------\n",
    "def tensor_stats(a: torch.Tensor):\n",
    "    a_np = a.numpy().ravel()\n",
    "    return {\n",
    "        \"mean\": float(a_np.mean()),\n",
    "        \"std\": float(a_np.std()),\n",
    "        \"min\": float(a_np.min()),\n",
    "        \"max\": float(a_np.max()),\n",
    "        \"abs_mean\": float(np.abs(a_np).mean())\n",
    "    }\n",
    "\n",
    "def compare_tensors(a: torch.Tensor, b: torch.Tensor, hist_bins=200) -> Dict:\n",
    "    # assume shapes equal (if not, skip)\n",
    "    a_np = a.numpy().ravel()\n",
    "    b_np = b.numpy().ravel()\n",
    "    l2 = float(np.linalg.norm(a_np - b_np))\n",
    "    rel_l2 = l2 / (float(np.linalg.norm(a_np)) + 1e-12)\n",
    "    max_abs = float(np.max(np.abs(a_np - b_np)))\n",
    "    if np.linalg.norm(a_np) < 1e-12 or np.linalg.norm(b_np) < 1e-12:\n",
    "        cosine_sim = 1.0 if np.allclose(a_np, b_np) else 0.0\n",
    "    else:\n",
    "        cosine_sim = float(np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np) + 1e-12))\n",
    "\n",
    "    # histogram KL (discretize)\n",
    "    # create shared bin edges covering both\n",
    "    mn = min(a_np.min(), b_np.min())\n",
    "    mx = max(a_np.max(), b_np.max())\n",
    "    if mn == mx:\n",
    "        kl = 0.0\n",
    "    else:\n",
    "        bins = np.linspace(mn, mx, hist_bins)\n",
    "        ha, _ = np.histogram(a_np, bins=bins, density=True)\n",
    "        hb, _ = np.histogram(b_np, bins=bins, density=True)\n",
    "        # add small epsilon to avoid zeros\n",
    "        ha += 1e-12\n",
    "        hb += 1e-12\n",
    "        kl = float(entropy(ha, hb))\n",
    "    return {\n",
    "        \"l2\": l2,\n",
    "        \"rel_l2\": rel_l2,\n",
    "        \"max_abs\": max_abs,\n",
    "        \"cosine\": cosine_sim,\n",
    "        \"kl_hist\": kl,\n",
    "        \"a_stats\": tensor_stats(a),\n",
    "        \"b_stats\": tensor_stats(b),\n",
    "    }\n",
    "\n",
    "# ---------- Layer-type heuristic ----------\n",
    "def layer_type_from_name(name: str) -> str:\n",
    "    # heuristic mapping for HF-like names. Update for your model naming scheme.\n",
    "    lname = name.lower()\n",
    "    if \"attn\" in lname or \"attention\" in lname or \".q_\" in lname or \".k_\" in lname or \".v_\" in lname:\n",
    "        return \"attention\"\n",
    "    if \"mlp\" in lname or \"feed_forward\" in lname or \"ffn\" in lname or \".fc\" in lname or \"dense\" in lname:\n",
    "        return \"feedforward\"\n",
    "    if \"layernorm\" in lname or \"ln\" in lname:\n",
    "        return \"layernorm\"\n",
    "    if \"embed\" in lname or \"token\" in lname:\n",
    "        return \"embedding\"\n",
    "    return \"other\"\n",
    "\n",
    "# ---------- Main comparison function ----------\n",
    "def compare_state_dicts(dict_a: Dict[str, torch.Tensor], dict_b: Dict[str, torch.Tensor]):\n",
    "    results = {}\n",
    "    for k, a in dict_a.items():\n",
    "        if k not in dict_b:\n",
    "            continue\n",
    "        b = dict_b[k]\n",
    "        if a.shape != b.shape:\n",
    "            # skip incompatible shapes (e.g., quantized packs weights differently).\n",
    "            continue\n",
    "        cmp = compare_tensors(a, b)\n",
    "        cmp[\"name\"] = k\n",
    "        cmp[\"shape\"] = tuple(a.shape)\n",
    "        cmp[\"layer_type\"] = layer_type_from_name(k)\n",
    "        results[k] = cmp\n",
    "    return results\n",
    "\n",
    "# ---------- Aggregation and reporting ----------\n",
    "def report_top_changes(results: Dict[str, Dict], top_k=25):\n",
    "    # Sort by rel_l2 or max_abs etc. Use multiple sorts for insight.\n",
    "    items = list(results.values())\n",
    "    by_rel = sorted(items, key=lambda x: x[\"rel_l2\"], reverse=True)\n",
    "    by_l2 = sorted(items, key=lambda x: x[\"l2\"], reverse=True)\n",
    "    by_max = sorted(items, key=lambda x: x[\"max_abs\"], reverse=True)\n",
    "    return {\"by_rel_l2\": by_rel[:top_k], \"by_l2\": by_l2[:top_k], \"by_max_abs\": by_max[:top_k]}\n",
    "\n",
    "def aggregate_by_type(results: Dict[str, Dict]):\n",
    "    agg = defaultdict(list)\n",
    "    for v in results.values():\n",
    "        agg[v[\"layer_type\"]].append(v)\n",
    "    summary = {}\n",
    "    for t, arr in agg.items():\n",
    "        summary[t] = {\n",
    "            \"count\": len(arr),\n",
    "            \"avg_rel_l2\": float(np.mean([x[\"rel_l2\"] for x in arr])),\n",
    "            \"avg_l2\": float(np.mean([x[\"l2\"] for x in arr])),\n",
    "            \"avg_max_abs\": float(np.mean([x[\"max_abs\"] for x in arr]))\n",
    "        }\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c525cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AI_HW\\DL_PRAC\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2792: RuntimeWarning: overflow encountered in dot\n",
      "  sqnorm = x.dot(x)\n",
      "c:\\AI_HW\\DL_PRAC\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:204: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "c:\\AI_HW\\DL_PRAC\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:193: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "c:\\AI_HW\\DL_PRAC\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:170: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top changed by relative L2:\n",
      "model.layers.14.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=2.4170e+03 | l2=3.7215e+04 | max_abs=1.2696e+02\n",
      "model.layers.1.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=2.3616e+03 | l2=3.6758e+04 | max_abs=1.2696e+02\n",
      "model.layers.4.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=2.3484e+03 | l2=3.2685e+04 | max_abs=1.2697e+02\n",
      "model.layers.8.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=2.1849e+03 | l2=3.2415e+04 | max_abs=1.2696e+02\n",
      "model.layers.13.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=2.1844e+03 | l2=3.6204e+04 | max_abs=1.2696e+02\n",
      "model.layers.10.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=2.0561e+03 | l2=3.2027e+04 | max_abs=1.2696e+02\n",
      "model.layers.15.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=2.0558e+03 | l2=3.7723e+04 | max_abs=1.2695e+02\n",
      "model.layers.3.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=1.9515e+03 | l2=3.0997e+04 | max_abs=1.2696e+02\n",
      "model.layers.2.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=1.9452e+03 | l2=3.6076e+04 | max_abs=1.2695e+02\n",
      "model.layers.16.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=1.8956e+03 | l2=3.7423e+04 | max_abs=1.2695e+02\n",
      "model.layers.17.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=1.8915e+03 | l2=3.4544e+04 | max_abs=1.2696e+02\n",
      "model.layers.6.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=1.8836e+03 | l2=3.2222e+04 | max_abs=1.2695e+02\n",
      "model.layers.7.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=1.7024e+03 | l2=3.2577e+04 | max_abs=1.2695e+02\n",
      "model.layers.3.self_attn.o_proj.weight                                                                                   | type=attention    | rel_l2=1.6050e+03 | l2=2.5804e+04 | max_abs=1.2695e+02\n",
      "model.layers.11.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=1.5877e+03 | l2=3.2605e+04 | max_abs=1.2694e+02\n",
      "model.layers.16.self_attn.o_proj.weight                                                                                  | type=attention    | rel_l2=1.5868e+03 | l2=2.7697e+04 | max_abs=1.2695e+02\n",
      "model.layers.9.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=1.5098e+03 | l2=3.1900e+04 | max_abs=1.2694e+02\n",
      "model.layers.1.self_attn.o_proj.weight                                                                                   | type=attention    | rel_l2=1.4670e+03 | l2=2.7138e+04 | max_abs=1.2694e+02\n",
      "model.layers.4.self_attn.o_proj.weight                                                                                   | type=attention    | rel_l2=1.4443e+03 | l2=2.4821e+04 | max_abs=1.2695e+02\n",
      "model.layers.0.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=1.4414e+03 | l2=3.5491e+04 | max_abs=1.2694e+02\n",
      "model.layers.12.mlp.down_proj.weight                                                                                     | type=feedforward  | rel_l2=1.4152e+03 | l2=3.2043e+04 | max_abs=1.2694e+02\n",
      "model.layers.8.self_attn.o_proj.weight                                                                                   | type=attention    | rel_l2=1.2842e+03 | l2=2.6978e+04 | max_abs=1.2695e+02\n",
      "model.layers.5.mlp.down_proj.weight                                                                                      | type=feedforward  | rel_l2=1.2737e+03 | l2=3.1587e+04 | max_abs=1.2693e+02\n",
      "model.layers.13.self_attn.o_proj.weight                                                                                  | type=attention    | rel_l2=1.1966e+03 | l2=2.6096e+04 | max_abs=1.2694e+02\n",
      "model.layers.6.self_attn.o_proj.weight                                                                                   | type=attention    | rel_l2=1.0882e+03 | l2=2.6560e+04 | max_abs=1.2695e+02\n",
      "model.layers.2.self_attn.o_proj.weight                                                                                   | type=attention    | rel_l2=1.0837e+03 | l2=2.6318e+04 | max_abs=1.2693e+02\n",
      "model.layers.17.self_attn.o_proj.weight                                                                                  | type=attention    | rel_l2=1.0614e+03 | l2=2.6006e+04 | max_abs=1.2693e+02\n",
      "model.layers.15.self_attn.o_proj.weight                                                                                  | type=attention    | rel_l2=1.0238e+03 | l2=2.6685e+04 | max_abs=1.2693e+02\n",
      "model.layers.5.self_attn.o_proj.weight                                                                                   | type=attention    | rel_l2=1.0060e+03 | l2=2.4976e+04 | max_abs=1.2693e+02\n",
      "model.layers.2.self_attn.q_proj.weight                                                                                   | type=attention    | rel_l2=9.7728e+02 | l2=3.0436e+04 | max_abs=1.2697e+02\n",
      "model.layers.14.self_attn.o_proj.weight                                                                                  | type=attention    | rel_l2=9.7186e+02 | l2=2.6857e+04 | max_abs=1.2692e+02\n",
      "model.layers.14.self_attn.k_proj.weight                                                                                  | type=attention    | rel_l2=9.6406e+02 | l2=1.5135e+04 | max_abs=1.2695e+02\n",
      "model.layers.15.mlp.up_proj.weight                                                                                       | type=feedforward  | rel_l2=9.5937e+02 | l2=4.2863e+04 | max_abs=1.2694e+02\n",
      "model.layers.15.mlp.gate_proj.weight                                                                                     | type=feedforward  | rel_l2=9.5426e+02 | l2=4.2650e+04 | max_abs=1.2692e+02\n",
      "model.layers.12.self_attn.o_proj.weight                                                                                  | type=attention    | rel_l2=9.5376e+02 | l2=2.5817e+04 | max_abs=1.2692e+02\n",
      "model.layers.16.mlp.up_proj.weight                                                                                       | type=feedforward  | rel_l2=9.5357e+02 | l2=4.2683e+04 | max_abs=1.2693e+02\n",
      "model.layers.16.self_attn.q_proj.weight                                                                                  | type=attention    | rel_l2=9.5325e+02 | l2=2.9371e+04 | max_abs=1.2698e+02\n",
      "model.layers.14.mlp.up_proj.weight                                                                                       | type=feedforward  | rel_l2=9.5250e+02 | l2=4.2495e+04 | max_abs=1.2695e+02\n",
      "model.layers.16.mlp.gate_proj.weight                                                                                     | type=feedforward  | rel_l2=9.5153e+02 | l2=4.2427e+04 | max_abs=1.2693e+02\n",
      "model.layers.2.mlp.up_proj.weight                                                                                        | type=feedforward  | rel_l2=9.4975e+02 | l2=4.2493e+04 | max_abs=1.2692e+02\n",
      "\n",
      "Aggregate by layer type:\n",
      "embedding    | count=  1 | avg_rel_l2=6.6679e-09 | avg_max=2.9802e-08\n",
      "layernorm    | count= 54 | avg_rel_l2=0.0000e+00 | avg_max=0.0000e+00\n",
      "feedforward  | count= 54 | avg_rel_l2=1.2444e+03 | avg_max=1.2694e+02\n",
      "attention    | count=126 | avg_rel_l2=5.3747e+02 | avg_max=7.2537e+01\n",
      "other        | count=  1 | avg_rel_l2=0.0000e+00 | avg_max=0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# example: replace these with your actual file paths or state_dicts\n",
    "orig_path = r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model\\model.safetensors\"   # FP16/FP32\n",
    "quant_path = r\"C:\\AI_HW\\DL_PRAC\\Gemma\\gemma3-270m\\model_8bitq\\model.safetensors\" # 8-bit/4-bit safetensors\n",
    "\n",
    "orig = load_safetensors_to_dict(orig_path)\n",
    "quant = load_safetensors_to_dict(quant_path)\n",
    "\n",
    "res = compare_state_dicts(orig, quant)\n",
    "top = report_top_changes(res, top_k=40)\n",
    "agg = aggregate_by_type(res)\n",
    "\n",
    "# print top changed layers by relative L2\n",
    "print(\"Top changed by relative L2:\")\n",
    "for r in top[\"by_rel_l2\"]:\n",
    "    print(f\"{r['name'][:120]:120} | type={r['layer_type']:12} | rel_l2={r['rel_l2']:.4e} | l2={r['l2']:.4e} | max_abs={r['max_abs']:.4e}\")\n",
    "\n",
    "print(\"\\nAggregate by layer type:\")\n",
    "for t, s in agg.items():\n",
    "    print(f\"{t:12} | count={s['count']:3} | avg_rel_l2={s['avg_rel_l2']:.4e} | avg_max={s['avg_max_abs']:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40899d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
